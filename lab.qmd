---
title: "Lab"
format: html
editor: visual
---

## Download the data

We will use AlphaVantage and the `tidyquant` package to get price history and other metadata. 

```{r}
# # ============================================================
# # CLEAN Alpha Vantage fetch (teaching version)
# # - Pulls DAILY ADJUSTED (full history)
# # - Builds long prices + wide returns CSVs for your BigVAR lab
# # ============================================================
# 
# library(tidyverse)
# library(lubridate)
# library(httr2)
# library(readr)
# 
# # 1) Universe: top 15 (keep GOOGL, add XOM, drop GOOG)
# TICKERS <- c(
#   "NVDA","AAPL","MSFT","AMZN","GOOGL","AVGO","META","TSLA","BRK.B",
#   "LLY","WMT","JPM","V","ORCL","XOM"
# )
# 
# # 2) Your API key (recommended: put this in .Renviron instead)
# #API_KEY <- Sys.getenv("ALPHAVANTAGE_API_KEY")
# # or: API_KEY <- "PASTE_YOUR_KEY_HERE"
# 
# # 3) Where to save outputs
# dir.create("data", showWarnings = FALSE)
# OUT_PRICES  <- "data/sp500_top15_prices_long.csv"
# OUT_RETURNS <- "data/sp500_top15_returns.csv"
# OUT_META    <- "data/sp500_top15_meta.csv"
# 
# # 4) Hard-coded metadata (sectors) for the 15 tickers
# meta <- tibble::tribble(
#   ~symbol, ~company, ~sector,
#   "NVDA",  "NVIDIA",                               "Information Technology",
#   "AAPL",  "Apple",                                "Information Technology",
#   "MSFT",  "Microsoft",                            "Information Technology",
#   "AMZN",  "Amazon",                               "Consumer Discretionary",
#   "GOOGL", "Alphabet (Class A)",                   "Communication Services",
#   "AVGO",  "Broadcom",                             "Information Technology",
#   "META",  "Meta Platforms",                       "Communication Services",
#   "TSLA",  "Tesla",                                "Consumer Discretionary",
#   "BRK.B", "Berkshire Hathaway (Class B)",         "Financials",
#   "LLY",   "Eli Lilly",                            "Health Care",
#   "WMT",   "Walmart",                              "Consumer Staples",
#   "JPM",   "JPMorgan Chase",                       "Financials",
#   "V",     "Visa",                                 "Financials",
#   "ORCL",  "Oracle",                               "Information Technology",
#   "XOM",   "Exxon Mobil",                          "Energy"
# )
# 
# readr::write_csv(meta, OUT_META)
# 
# # ------------------------------------------------------------
# # Function: download ONE ticker from Alpha Vantage (CSV)
# # ------------------------------------------------------------
# fetch_daily_adjusted <- function(symbol, api_key) {
#   req <- request("https://www.alphavantage.co/query") |>
#     req_url_query(
#       `function`  = "TIME_SERIES_DAILY_ADJUSTED",
#       symbol      = symbol,
#       outputsize  = "full",
#       datatype    = "csv",
#       apikey      = api_key
#     )
# 
#   # CSV comes back as text; read it into a tibble
#   txt <- req |> req_perform() |> resp_body_string()
#   readr::read_csv(I(txt), show_col_types = FALSE) |>
#     mutate(symbol = symbol)
# }
# 
# # ------------------------------------------------------------
# # 5) Fetch all tickers (simple loop)
# # ------------------------------------------------------------
# # Note: Alpha Vantage rate-limits. For teaching, we just sleep a bit.
# prices_long <- map_dfr(TICKERS, function(sym) {
#   message("Fetching ", sym, " ...")
#   Sys.sleep(.5)   # adjust for your key/limits
#   fetch_daily_adjusted(sym, API_KEY)
# })
# 
# # Keep only the fields we care about, and rename timestamp -> date
# prices_long <- prices_long |>
#   transmute(
#     symbol,
#     date = as.Date(timestamp),
#     adjusted = adjusted_close
#   ) |>
#   arrange(symbol, date)
# 
# readr::write_csv(prices_long, OUT_PRICES)
# 
# # ------------------------------------------------------------
# # 6) Compute eturns from adjusted prices
# # ------------------------------------------------------------
# returns_long <- prices_long |>
#   group_by(symbol) |>
#   arrange(date, .by_group = TRUE) |>
#   mutate(ret = adjusted / lag(adjusted) - 1) |>
#   ungroup() |>
#   filter(!is.na(ret)) |>
#   select(date, symbol, ret)
# 
# # Wide returns, strict intersection of dates across tickers
# returns_wide <- returns_long |>
#   pivot_wider(names_from = symbol, values_from = ret) |>
#   arrange(date) |>
#   tidyr::drop_na(all_of(TICKERS))
# 
# readr::write_csv(returns_wide, OUT_RETURNS)
# 
# # ------------------------------------------------------------
# # 7) Quick check
# # ------------------------------------------------------------
# message("Wrote: ", OUT_META)
# message("Wrote: ", OUT_PRICES)
# message("Wrote: ", OUT_RETURNS)
# message("Date range: ", min(returns_wide$date), " to ", max(returns_wide$date))
# message("Rows (aligned trading days): ", nrow(returns_wide))


```


## Load Data

```{r}
# ============================================================
# BigVAR + Sector jIRF Lab (clean student version)
# - Input files (provided by instructor):
#     data/sp500_top15_returns.csv   (wide; simple returns)
#     data/sp500_top15_meta.csv      (symbol, company, sector)
# ============================================================

library(tidyverse)
library(BigVAR)

# -----------------------------
# 0) Inputs + knobs
# -----------------------------
PRICES_
RETURNS_CSV <- "data/sp500_top15_returns.csv"
META_CSV    <- "data/sp500_top15_meta.csv"

P_LAGS  <- 5            # VAR order
STRUCT  <- "Basic"      # regularization structure (start simple)
GRAN    <- c(150, 10)   # lambda grid: (path length, # grid points)
H_IRF   <- 21           # horizons: 0..20
SHOCK_SD <- 2           # 2-SD joint shocks

# -----------------------------
# 1) Load data
# -----------------------------
returns_wide <- readr::read_csv(RETURNS_CSV, show_col_types = FALSE) %>%
  mutate(date = as.Date(date)) %>%
  arrange(date)

meta <- readr::read_csv(META_CSV, show_col_types = FALSE) %>%
  mutate(symbol = as.character(symbol))

TICKERS <- meta$symbol

Y <- returns_wide %>%
  select(all_of(TICKERS)) %>%
  as.matrix()

returns_wide
meta
Y
```

## Prep for fitting BigVar

- define the train/test split
- standardize the returns based on training set statistics

```{r}
# -----------------------------
# 2) Standardize returns (train-based)
#    (Gives “2 SD” shocks a consistent meaning across series)
# -----------------------------
Tn <- nrow(Y)
K  <- ncol(Y)

T1 <- floor(0.60 * Tn)
T2 <- floor(0.80 * Tn)

train_idx <- 1:(T1 - 1)

mu <- colMeans(Y[train_idx, , drop = FALSE])
sdv <- apply(Y[train_idx, , drop = FALSE], 2, sd)

Y_sc <- scale(Y, center = mu, scale = sdv) %>% as.matrix()

scale(Y, center = mu, scale = sdv) %>% 
  ggplot(aes(x = NVDA)) + geom_histogram()

Y_sc


```

## Fit the BigVar with rolling Cross validation and lasso regularization

```{r}
# -----------------------------
# 3) Fit BigVAR with rolling CV
# -----------------------------
mod <- constructModel(
  Y_sc,
  p      = P_LAGS,
  struct = STRUCT,
  gran   = GRAN,
  h      = 1,
  cv     = "Rolling",
  T1     = T1,
  T2     = T2,
  verbose = FALSE,
  IC = FALSE,
  model.controls = list(intercept = FALSE)
)

fit <- cv.BigVAR(mod)
print(fit)

# Sparsity plot (built-in)
SparsityPlot.BigVAR.results(fit)

B <- as.matrix(fit@betaPred)

# fraction exactly zero
mean(B == 0)

# how many nonzero coefficients
sum(B != 0)

# biggest coefficients
sort(abs(as.vector(B)), decreasing = TRUE)[1:20]

```

